\documentclass[conference]{IEEEtran}
%\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

%\addtolength{\oddsidemargin}{-.875in}
%\addtolength{\evensidemargin}{-.875in}
%\addtolength{\textwidth}{1.75in}
%\addtolength{\topmargin}{-.875in}
%\addtolength{\textheight}{1.75in}
	
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{url}
\usepackage{float}
\usepackage{setspace}
\usepackage{filecontents,lipsum}
\usepackage[noadjust]{cite}

\begin{document}
%\raggedright
%\doublespacing

\title{Week 2 Journal}
\author{Rodger Byrd}
\maketitle


\section{Process}

For this Journal, I used Zotero to track all of my bibliography entries and notes. It was significantly faster and easier than what I did in Journal 1. 
For the "top" journal I chose IET Information Security \cite{noauthor_iet_nodate}. It had 5 issues in 2019 with 10-12 articles per issue. My filed of study is related to both Compute Science and Computer Security so I thought this would include relevant papers for me to read. My process included 3 phases. 
First, quickly browse each paper using my phone as a stopwatch to determine if I wanted to scan or trash the paper with a note about why.
Secondly, I did a 5-10 minute scan of the papers chosen in phase 1 to determine if I wanted to critically and creatively read them.
Lastly, I chose the best 2 papers and read them tracking my notes. The notes are included in the next section. 
The files for this latex document are in the github repository located at \path{https://github.com/rodger79/CS6000}.

\section{Raw Notes}
For my first detailed read I chose a paper called  \textit{Re-definable access control over outsourced data in cloud storage systems} \cite{zhang_re-definable_2019}.The following are my raw notes for this paper.


Authors propose RDAC as approach to secure outsourced data and allow access control. Interesting idea to take encrypted data and somehow conver that to another type of encrypted data that could be decrypted by authorized users based on different criteria than original encryptor.
How does the IBE (identity based encryption) actually convert to ABE (attribute based encryption). 
Seems non-trivial.
I can see how a company would use ABE or IBE/PKI, curious to see how they propose to convert from one to the other. 
conversion keys? Multiple pages on how great ABE and IBE are, but they are proposing RDAC (re-difinable access control). Use a bunch of set theory to define who should get access to what.
Looks like a proxy server will decide whether to decrypt data out of IBE for approved users
Even more detail on IBE and ABE in section 5.3.
6.1 Basic idea, just restate everying already talked about in detail, waste of a section.
Looking back at the system architecture, it doesn't show hot the Trusted Authority would authorize the proxy do do the "re-encryption"?
They are finally talking about a master secret key (msk). 
finally, section 6.2.4 File conversion.
Still claiming that it doesn't need to be decrypted, just converted to another encryption, i'm not convinced, hopefully they built some actual system.
Experimental section 8.2 included more information about performance of different encryption then the conversion process. 
I think this is more of a toy example. I'd like to see something more comphrensive then a few small tests on a mobile device and PC.
Overall, dissapointed in this paper, although I can see how the concept is very relevant with so many companies outsourcing to the cloud. 
They could have done a better job explaining the implementation and doing performance testing.


For my next detailed read I chose a paper called  \textit{Causal analysis of attacks against honeypots based on properties of countries} \cite{zuzcak_causal_2019}. The following are my raw notes for this paper.

After setting up a honeypot, the authors looks at the demographic, technical, and economic data of the countries the attacks originated from. This could be interesting from a threat perspective, as-in can you use the country of request origination to determine relevant threat levels.
They state they are attempting to clarify the relation between the number of attacks and their countries of origin. 
Set up a "honeynet", collection of 7 honeypots directly on the internet for 1 year.
Big assumption that you can actually determine an attacker from their IP address, unclear how they actually determined this other than they say they used databases in 3.3. 
Oh, the databases weren't for location information they were for exonomic data/demographic data
With VPNs and Tor, I seriously question whether any statistical analysis is even valid.
A lot of sections on statistics.
Seems like they just gathered a bunch of data then tried to write a paper after.
Meaning come up with the hypothesis later.
I wonder if you could do a second analysis overlaying where Tor is banned to see how that affects the countries of origin.
For instance, they claim the Netherlands have an unusual amount of attacks coming from them, do they also have a lot of Tor nodes?
I think the idea behind this paper is interesting, but they don't mention once about VPNs. 
Huge potential problem

Papers that were scanned and trashed are referenced in the bibliography below. 
\nocite{*}

\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}